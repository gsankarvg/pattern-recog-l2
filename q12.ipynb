{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621f12bb",
   "metadata": {},
   "source": [
    "12. Implement  the  Q-learning  algorithm  in  Python  for  a  simple  5x5  grid  world \n",
    "environment for an agent to reach the bottom-right cell as its goal starting from the \n",
    "top-left  position  and  with  action  space defined as {up, down, right, left}. Use this \n",
    "implementation to find the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa728c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d2a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5\n",
    "actions = [\"up\",\"down\",\"left\",\"right\"]\n",
    "n_actions = len(actions)\n",
    "goal_state = (4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafd9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_index(state):\n",
    "    return state[0] * grid_size + state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e57c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_state(index):\n",
    "    return (index // grid_size, index % grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f14f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(state):\n",
    "    return 0 <= state[0] < grid_size and 0 <= state[1] < grid_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c0871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    x, y = state\n",
    "    if action == 0:\n",
    "         x -= 1          # up\n",
    "    elif action == 1:\n",
    "         x += 1          # down\n",
    "    elif action == 2:\n",
    "         y -= 1          # left\n",
    "    elif action == 3:\n",
    "         y += 1          # right\n",
    "    next_state = (x, y)\n",
    "    return next_state if is_valid(next_state) else state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bde8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = grid_size * grid_size\n",
    "q_table = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ebcc3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1       # learning rate (how much to update Q-values)\n",
    "gamma = 0.9       # discount factor (importance of future rewards)\n",
    "epsilon = 0.2     # exploration rate (how often to explore)\n",
    "episodes = 1000   # total times the agent will try to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state = (0,0)\n",
    "    while state != goal_state:\n",
    "        s_idx = state_to_index(state)\n",
    "\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, n_actions - 1) \n",
    "        else:\n",
    "            action = np.argmax(q_table[s_idx])  \n",
    "        \n",
    "        next_state = get_next_state(state, action)\n",
    "        next_s_idx = state_to_index(next_state)\n",
    "        reward = 10 if next_state == goal_state else -1\n",
    "\n",
    "        q_table[s_idx, action] += alpha * (\n",
    "            reward + gamma * np.max(q_table[next_s_idx]) - q_table[s_idx, action]\n",
    "        )\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34243c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['down' 'down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'right' 'down']\n",
      " ['right' 'right' 'down' 'right' 'down']\n",
      " ['right' 'right' 'down' 'down' 'down']\n",
      " ['right' 'right' 'right' 'right' 'up']]\n"
     ]
    }
   ],
   "source": [
    "policy = np.array([''] * n_states, dtype=object)\n",
    "\n",
    "for state_index in range(n_states):\n",
    "    best_action = np.argmax(q_table[state_index])\n",
    "    policy[state_index] = actions[best_action]\n",
    "\n",
    "policy_grid = policy.reshape((grid_size, grid_size))\n",
    "print(policy_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
